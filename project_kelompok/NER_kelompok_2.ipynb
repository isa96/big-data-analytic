{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mhihsan/abd/blob/main/project_kelompok/NER_kelompok_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TA21Jo5d9SVq"
      },
      "source": [
        "\n",
        "\n",
        "![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/healthcare/NER_LAB.ipynb)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzIdjHkAW8TB"
      },
      "source": [
        "# **Detect lab results**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uDmeHEFW7_h"
      },
      "source": [
        "To run this yourself, you will need to upload your license keys to the notebook. Just Run The Cell Below in order to do that. Also You can open the file explorer on the left side of the screen and upload `license_keys.json` to the folder that opens.\n",
        "Otherwise, you can look at the example outputs at the bottom of the notebook.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIeCOiJNW-88"
      },
      "source": [
        "## 1. Colab Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMIDv74CYN0d"
      },
      "source": [
        "Import license keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72,
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "ok": true,
              "status": 200,
              "status_text": ""
            }
          }
        },
        "id": "ttHPIV2JXbIM",
        "outputId": "b060d764-3610-4e82-8bf3-4541193ffab2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7dd961d7-4fc8-4c82-8c0d-38f52ae07ced\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-7dd961d7-4fc8-4c82-8c0d-38f52ae07ced\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving spark_nlp_for_healthcare_spark_ocr_4546.json to spark_nlp_for_healthcare_spark_ocr_4546.json\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "license_keys = files.upload()\n",
        "\n",
        "with open(list(license_keys.keys())[0]) as f:\n",
        "    license_keys = json.load(f)\n",
        "\n",
        "# Defining license key-value pairs as local variables\n",
        "locals().update(license_keys)\n",
        "\n",
        "# Adding license key-value pairs to environment variables\n",
        "os.environ.update(license_keys)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1CSHKAYywFDY",
        "outputId": "b4d15d24-4609-40b1-8e75-12dd11cf2d01"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'AWS_ACCESS_KEY_ID': 'ASIASRWSDKBGAF5QO7NP',\n",
              " 'AWS_SECRET_ACCESS_KEY': 'g4pjHRVwkjQ6KtWuv+GWVcKSpKA4BzSkbeQYXQpK',\n",
              " 'AWS_SESSION_TOKEN': 'IQoJb3JpZ2luX2VjEJD//////////wEaCXVzLWVhc3QtMSJIMEYCIQDfZlg6+xWQxtrwV2xlaQaFQhqvCXlHH/JxGug6InTpugIhALrRY4Rz2nFTF0K3Rwj1eEuCaA2t5bzrbZdt6luv0Fl8KscCCBgQAxoMMTc1NDYwNTM2Mzk2IgzyoH1rfA+leTca+v4qpAIhMv2BGhU77UaYfk/NL4fkzqqnZ/tBUUlqnO2BtBqeocgCSJVvFWtUFggrFbiAF845R2BCw15xG53CC0EMp36P0YUzSwWPzXbfYtvkG2sx+Jypi8B673AykmALLL9U0kJ3dANTFpeh2peF8BrCsJxGRRBr907mc+EioEWfdZD0Tv+uP9vMhav9nt7/wALdXJZepNE4POHQXP3qgGbk+ymQ7QicZ4EcEiGe2ptwj7WvNwY/0KvNT+Q9YqRCf+y5k2t0m3GyrUVy3E3eNYDZ4DW1KWyFPBi+hcgNtbW8sOpNfj2WOFsnedXoEICmDWPC7Vl4nUgKBB/nri1ZRoWRQrDzOpc6/HIJoctYjMFiYgH2kl0+fRLZLlRvtyHpmgF59O0A+Kn1MI+o4pEGOtUB0kKglASpG08vDg/m0RSzGosQTZpltSMsD1osT3iOhO3f5tRoOqsIZVhfBXiuOqASauZnNMrcrfOzpcwa9Rs/fcDZc3AJBvxi2k+uT00WvaaqfviJ8kPzOqEaziaLSbPXUrqVp03WgimerV8gJ/p+AOQIzKo7WYgQJ+Fi3k0u/lTbskGukCVVkQkHB2QQ2HAolucN8LdKSsFpwgx7JTiJTPFBHv2gqR2Vj0LnSZIySrfwbL+PFOP+++lOd1n0dWk3NUnQwMb3+TKTAWdqemLwELtkK7rk',\n",
              " 'JSL_VERSION': '3.4.2',\n",
              " 'OCR_VERSION': '3.11.0',\n",
              " 'PUBLIC_VERSION': '3.4.2',\n",
              " 'SECRET': '3.4.2-646f0f9bfd1342f35e98154bab255b85ac9830a3',\n",
              " 'SPARK_NLP_LICENSE': 'eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJleHAiOjE2NTA0NjY1NDgsImlhdCI6MTY0Nzg3NDU0OCwidW5pcXVlX2lkIjoiZmUyY2Y5Y2MtYTkyNi0xMWVjLWFmOWQtOGEzNWI0NjA2MzgxIiwic2NvcGUiOlsib2NyOmluZmVyZW5jZSIsIm9jcjp0cmFpbmluZyIsImhlYWx0aGNhcmU6aW5mZXJlbmNlIiwiaGVhbHRoY2FyZTp0cmFpbmluZyJdLCJwbGF0Zm9ybSI6eyJuYW1lIjoiRmxvYXRpbmciLCJ0YWJsZV9uYW1lIjoiTGljZW5zZUxvY2tzIn19.GAlxz6gJKWrHac56_tdKlNBZ_mo8zc1zutsto9sR3EWl-x1oRmKoQO6LuROmOtRA_RxuYnG_8Z1VnBzgw6JK9A1UGGVlLwQkgQBdOVVNdPHVaXANAJIynfA__g5fr9Enc1LJfDVom0HrN83yX6hNYfqSNQXGxQm3hDlOhUKGFbMROtr5K06xg4r-zwD-tCAp0HO5kkme4ob7KacWEl7aMFfM38RqAnimbZncoYYiQVlEk3t5REqEgdFPHuk22FHgrpEDCH8y_qBpGAD9YX9VJzOcRVYdZS2Ec6kS1YMVr_1Sw4JlzVm-IwZHSbyBYIUETkyb9x4HnDwjOvSAqOPbvg',\n",
              " 'SPARK_OCR_LICENSE': 'eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJleHAiOjE2NTA0NjY1NDgsImlhdCI6MTY0Nzg3NDU0OCwidW5pcXVlX2lkIjoiZmUyY2Y5Y2MtYTkyNi0xMWVjLWFmOWQtOGEzNWI0NjA2MzgxIiwic2NvcGUiOlsib2NyOmluZmVyZW5jZSIsIm9jcjp0cmFpbmluZyIsImhlYWx0aGNhcmU6aW5mZXJlbmNlIiwiaGVhbHRoY2FyZTp0cmFpbmluZyJdLCJwbGF0Zm9ybSI6eyJuYW1lIjoiRmxvYXRpbmciLCJ0YWJsZV9uYW1lIjoiTGljZW5zZUxvY2tzIn19.GAlxz6gJKWrHac56_tdKlNBZ_mo8zc1zutsto9sR3EWl-x1oRmKoQO6LuROmOtRA_RxuYnG_8Z1VnBzgw6JK9A1UGGVlLwQkgQBdOVVNdPHVaXANAJIynfA__g5fr9Enc1LJfDVom0HrN83yX6hNYfqSNQXGxQm3hDlOhUKGFbMROtr5K06xg4r-zwD-tCAp0HO5kkme4ob7KacWEl7aMFfM38RqAnimbZncoYYiQVlEk3t5REqEgdFPHuk22FHgrpEDCH8y_qBpGAD9YX9VJzOcRVYdZS2Ec6kS1YMVr_1Sw4JlzVm-IwZHSbyBYIUETkyb9x4HnDwjOvSAqOPbvg',\n",
              " 'SPARK_OCR_SECRET': '3.11.0-fbf210603348ae29d22d900d1a894808c20ee6e4'}"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "license_keys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQtc1CHaYQjU"
      },
      "source": [
        "Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGJktFHdHL1n",
        "outputId": "d9d4512d-5691-42ef-e25e-b0850b50238e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 212.4 MB 67 kB/s \n",
            "\u001b[K     |████████████████████████████████| 142 kB 22.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 198 kB 63.0 MB/s \n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 148 kB 8.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 95 kB 2.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 66 kB 4.8 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Installing pyspark and spark-nlp\n",
        "! pip install --upgrade -q pyspark==3.1.2 spark-nlp==$PUBLIC_VERSION\n",
        "\n",
        "# Installing Spark NLP Healthcare\n",
        "! pip install --upgrade -q spark-nlp-jsl==$JSL_VERSION  --extra-index-url https://pypi.johnsnowlabs.com/$SECRET\n",
        "\n",
        "# Installing Spark NLP Display Library for visualization\n",
        "! pip install -q spark-nlp-display"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hj5FRDV4YSXN"
      },
      "source": [
        "Import dependencies into Python and start the Spark session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "sw-t1zxlHTB7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "import sparknlp\n",
        "from sparknlp.annotator import *\n",
        "from sparknlp_jsl.annotator import *\n",
        "from sparknlp.base import *\n",
        "import sparknlp_jsl\n",
        "\n",
        "spark = sparknlp_jsl.start(license_keys['SECRET'], gpu=True) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vrexwu-iDE6s"
      },
      "source": [
        "### Data Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Z3oqA5CmDF1b",
        "outputId": "e2a57099-e623-4f32-e80c-59decfb8c8ea"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ff44645b-6b14-47af-9030-a6258f6498ac\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ff44645b-6b14-47af-9030-a6258f6498ac\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# upload dataset mtsamples\n",
        "f = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "Nhf7bGv_DNyI",
        "outputId": "912b5958-e219-47a9-e218-3bce0a225c49"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-7c76b675-5ac8-4e66-8479-6720658dd52a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>medical_speciality</th>\n",
              "      <th>sample_name</th>\n",
              "      <th>description</th>\n",
              "      <th>transcription</th>\n",
              "      <th>keywords</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Allergy / Immunology</td>\n",
              "      <td>Allergic Rhinitis</td>\n",
              "      <td>A 23-year-old white female presents with compl...</td>\n",
              "      <td>SUBJECTIVE: This 23-year-old white female pres...</td>\n",
              "      <td>allergy / immunology, allergic rhinitis, aller...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Allergy / Immunology</td>\n",
              "      <td>Allergy Evaluation Consult</td>\n",
              "      <td>Acute allergic reaction, etiology uncertain, h...</td>\n",
              "      <td>HISTORY: A 34-year-old male presents today sel...</td>\n",
              "      <td>allergy / immunology, keflex, acute allergic r...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Allergy / Immunology</td>\n",
              "      <td>Asthma in a 5-year-old</td>\n",
              "      <td>Mother states he has been wheezing and coughing.</td>\n",
              "      <td>CHIEF COMPLAINT: This 5-year-old male presents...</td>\n",
              "      <td>allergy / immunology, breathing treatment, air...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Allergy / Immunology</td>\n",
              "      <td>Chronic Sinusitis</td>\n",
              "      <td>Patient having severe sinusitis about two to t...</td>\n",
              "      <td>HISTORY: I had the pleasure of meeting and eva...</td>\n",
              "      <td>allergy / immunology, nasal congestion, facial...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7c76b675-5ac8-4e66-8479-6720658dd52a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7c76b675-5ac8-4e66-8479-6720658dd52a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7c76b675-5ac8-4e66-8479-6720658dd52a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     medical_speciality                 sample_name  \\\n",
              "0  Allergy / Immunology           Allergic Rhinitis   \n",
              "1  Allergy / Immunology  Allergy Evaluation Consult   \n",
              "2  Allergy / Immunology      Asthma in a 5-year-old   \n",
              "3  Allergy / Immunology           Chronic Sinusitis   \n",
              "\n",
              "                                         description  \\\n",
              "0  A 23-year-old white female presents with compl...   \n",
              "1  Acute allergic reaction, etiology uncertain, h...   \n",
              "2   Mother states he has been wheezing and coughing.   \n",
              "3  Patient having severe sinusitis about two to t...   \n",
              "\n",
              "                                       transcription  \\\n",
              "0  SUBJECTIVE: This 23-year-old white female pres...   \n",
              "1  HISTORY: A 34-year-old male presents today sel...   \n",
              "2  CHIEF COMPLAINT: This 5-year-old male presents...   \n",
              "3  HISTORY: I had the pleasure of meeting and eva...   \n",
              "\n",
              "                                            keywords  \n",
              "0  allergy / immunology, allergic rhinitis, aller...  \n",
              "1  allergy / immunology, keflex, acute allergic r...  \n",
              "2  allergy / immunology, breathing treatment, air...  \n",
              "3  allergy / immunology, nasal congestion, facial...  "
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# read csv to pandas\n",
        "import pandas as pd\n",
        "df = pd.read_csv('/content/mtsamples_20220222.csv')\n",
        "df = df.dropna()\n",
        "df = df.drop_duplicates()\n",
        "df.head(4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "h4_YfyePIDxw",
        "outputId": "694779e1-5cbd-4589-dcd4-0bb0e7c1fbf2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['\"', '#', '$', '&', \"'\", '(', ')', '*', '+', ',', '.', ':', ';', '<', '=', '>', '?', '@', '[', ']', '_', '{', '|', '}', '©', '®', '°', 'µ', '·', 'º', '¼', '½', 'è', 'é', 'ü', '–', '’', '“', '”', '…']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
            "  del sys.path[0]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-e624277c-91e5-4e39-aaf4-2a9dd99814f9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>medical_speciality</th>\n",
              "      <th>sample_name</th>\n",
              "      <th>description</th>\n",
              "      <th>transcription</th>\n",
              "      <th>keywords</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Allergy / Immunology</td>\n",
              "      <td>Allergic Rhinitis</td>\n",
              "      <td>A 23-year-old white female presents with compl...</td>\n",
              "      <td>SUBJECTIVE: This 23-year-old white female pres...</td>\n",
              "      <td>allergy / immunology, allergic rhinitis, aller...</td>\n",
              "      <td>subjective this 23-year-old white female prese...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Allergy / Immunology</td>\n",
              "      <td>Allergy Evaluation Consult</td>\n",
              "      <td>Acute allergic reaction, etiology uncertain, h...</td>\n",
              "      <td>HISTORY: A 34-year-old male presents today sel...</td>\n",
              "      <td>allergy / immunology, keflex, acute allergic r...</td>\n",
              "      <td>history a 34-year-old male presents today self...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Allergy / Immunology</td>\n",
              "      <td>Asthma in a 5-year-old</td>\n",
              "      <td>Mother states he has been wheezing and coughing.</td>\n",
              "      <td>CHIEF COMPLAINT: This 5-year-old male presents...</td>\n",
              "      <td>allergy / immunology, breathing treatment, air...</td>\n",
              "      <td>chief complaint this 5-year-old male presents ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Allergy / Immunology</td>\n",
              "      <td>Chronic Sinusitis</td>\n",
              "      <td>Patient having severe sinusitis about two to t...</td>\n",
              "      <td>HISTORY: I had the pleasure of meeting and eva...</td>\n",
              "      <td>allergy / immunology, nasal congestion, facial...</td>\n",
              "      <td>history i had the pleasure of meeting and eval...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Allergy / Immunology</td>\n",
              "      <td>Evaluation of Allergies</td>\n",
              "      <td>Chronic glossitis, xerostomia, probable enviro...</td>\n",
              "      <td>HISTORY: A 55-year-old female presents self-re...</td>\n",
              "      <td>allergy / immunology, chronic glossitis, xeros...</td>\n",
              "      <td>history a 55-year-old female presents self-ref...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e624277c-91e5-4e39-aaf4-2a9dd99814f9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e624277c-91e5-4e39-aaf4-2a9dd99814f9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e624277c-91e5-4e39-aaf4-2a9dd99814f9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     medical_speciality                 sample_name  \\\n",
              "0  Allergy / Immunology           Allergic Rhinitis   \n",
              "1  Allergy / Immunology  Allergy Evaluation Consult   \n",
              "2  Allergy / Immunology      Asthma in a 5-year-old   \n",
              "3  Allergy / Immunology           Chronic Sinusitis   \n",
              "4  Allergy / Immunology     Evaluation of Allergies   \n",
              "\n",
              "                                         description  \\\n",
              "0  A 23-year-old white female presents with compl...   \n",
              "1  Acute allergic reaction, etiology uncertain, h...   \n",
              "2   Mother states he has been wheezing and coughing.   \n",
              "3  Patient having severe sinusitis about two to t...   \n",
              "4  Chronic glossitis, xerostomia, probable enviro...   \n",
              "\n",
              "                                       transcription  \\\n",
              "0  SUBJECTIVE: This 23-year-old white female pres...   \n",
              "1  HISTORY: A 34-year-old male presents today sel...   \n",
              "2  CHIEF COMPLAINT: This 5-year-old male presents...   \n",
              "3  HISTORY: I had the pleasure of meeting and eva...   \n",
              "4  HISTORY: A 55-year-old female presents self-re...   \n",
              "\n",
              "                                            keywords  \\\n",
              "0  allergy / immunology, allergic rhinitis, aller...   \n",
              "1  allergy / immunology, keflex, acute allergic r...   \n",
              "2  allergy / immunology, breathing treatment, air...   \n",
              "3  allergy / immunology, nasal congestion, facial...   \n",
              "4  allergy / immunology, chronic glossitis, xeros...   \n",
              "\n",
              "                                                text  \n",
              "0  subjective this 23-year-old white female prese...  \n",
              "1  history a 34-year-old male presents today self...  \n",
              "2  chief complaint this 5-year-old male presents ...  \n",
              "3  history i had the pleasure of meeting and eval...  \n",
              "4  history a 55-year-old female presents self-ref...  "
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# concat sample_name, description, and transcription columns, then lower\n",
        "# df['text'] = df.apply(lambda x: x['sample_name'] +' '+ x['description'] +' '+ x['transcription'], axis=1).str.lower()\n",
        "\n",
        "# only use transcription columns for input  \n",
        "df['text'] = df.apply(lambda x: x['transcription'], axis=1).str.lower()\n",
        "\n",
        "# a little bit preprocesing delete unknown characters\n",
        "text_to_filter = ' '.join(df.text.values.flatten().tolist())\n",
        "import re, numpy as np\n",
        "excluded = [ st for st in np.unique(re.findall(r'.', text_to_filter)) if not re.search(r'[a-z]', st) and not re.search(r'[0-9]', st) and st not in ['%','/','-','!',' ']]\n",
        "print(excluded)\n",
        "for exc in excluded:\n",
        "    df['text'] = df['text'].str.replace(exc, '')\n",
        "df['text'] = df['text'].str.replace('\\n', ' ')\n",
        "df['text'] = df['text'].astype(str)\n",
        "df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifnm7l6r9agl"
      },
      "outputs": [],
      "source": [
        "# get 10 random samples for each top 25 medical_speciality  \n",
        "df = pd.concat([df[df.medical_speciality==_].sample(10, random_state=10) for _ in df.medical_speciality.value_counts()[:25].index], axis=0).reset_index(drop=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u87XVwH1JJ2U",
        "outputId": "fb74ab00-203f-43d5-fe02-70fcc6d64267"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(250, 1)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# save to csv on column text \n",
        "df[['text']].to_csv('data.csv', index=False)\n",
        "pd.read_csv('data.csv').shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRsLTSpr2OZD",
        "outputId": "08609beb-8467-4239-a1bf-ff5edbb3a980"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Surgery                          10\n",
              "Neurosurgery                     10\n",
              "Dermatology                      10\n",
              "Podiatry                         10\n",
              "Office Notes                     10\n",
              "Psychiatry / Psychology          10\n",
              "Pain Management                  10\n",
              "Pediatrics - Neonatal            10\n",
              "Emergency Room Reports           10\n",
              "Nephrology                       10\n",
              "Ophthalmology                    10\n",
              "Hematology - Oncology            10\n",
              "ENT - Otolaryngology             10\n",
              "Consult - History and Phy.       10\n",
              "Discharge Summary                10\n",
              "Urology                          10\n",
              "Obstetrics / Gynecology          10\n",
              "SOAP / Chart / Progress Notes    10\n",
              "Neurology                        10\n",
              "Gastroenterology                 10\n",
              "General Medicine                 10\n",
              "Radiology                        10\n",
              "Orthopedic                       10\n",
              "Cardiovascular / Pulmonary       10\n",
              "Dentistry                        10\n",
              "Name: medical_speciality, dtype: int64"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.medical_speciality.value_counts()[:25]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgTUYzDqJ5fl"
      },
      "source": [
        "### Load data to spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0OT_fpRJ8O0",
        "outputId": "fa3a8c69-a437-4268-8c7f-0d9382fb13e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------------------------------------------------------------------------------------+\n",
            "|                                                                                      text|\n",
            "+------------------------------------------------------------------------------------------+\n",
            "|procedure total hip replacement  procedure description the patient was bought to the op...|\n",
            "|procedure performed cataract extraction with lens implantation right eye  description o...|\n",
            "|preop diagnoses 1 left pilon fracture 2 left great toe proximal phalanx fracture  posto...|\n",
            "|preoperative diagnosis recurrent bladder tumors  postoperative diagnosis recurrent blad...|\n",
            "|preoperative diagnosis degenerative disk disease at l4-l5 and l5-s1  postoperative diag...|\n",
            "|the patient was consented for skin biopsy the complications instructions as to how the ...|\n",
            "|preoperative diagnosis varicose veins  postoperative diagnosis varicose veins  procedur...|\n",
            "|preoperative diagnosis t12 compression fracture with cauda equina syndrome and spinal c...|\n",
            "|preoperative diagnoses 1 dysphagia 2 right parapharyngeal hemorrhagic lesion  postopera...|\n",
            "|admitting diagnosis right c5-c6 herniated nucleus pulposus  primary operative procedure...|\n",
            "|reason for consultation mesothelioma  history of present illness the patient is a 73-ye...|\n",
            "|reason for referral the patient was referred to me by dr x of the hospitalist service a...|\n",
            "|history of present illness this is the initial clinic visit for a 41-year-old worker wh...|\n",
            "|cc slowing of motor skills and cognitive function  hx this 42 y/o lhm presented on 3/16...|\n",
            "|history of present illness the patient is an 85-year-old gentleman who follows as an ou...|\n",
            "|history neurologic consultation was requested to assess and assist with her seizure med...|\n",
            "|type of consultation wound care consult  history of present illness the patient is a 62...|\n",
            "|the patient is a 51-year-old female who was undergoing a routine physical examination a...|\n",
            "|chief complaint palpitations  chest pain / unspecified angina pectoris history the pati...|\n",
            "|reason for referral the patient is a 76-year-old caucasian gentleman who works full-tim...|\n",
            "+------------------------------------------------------------------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "250"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = spark.read.csv('data.csv', header=True, inferSchema=True)\n",
        "data.show(truncate=90)\n",
        "data.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iK9LqdQGQD7O"
      },
      "outputs": [],
      "source": [
        "text_col = 'text'\n",
        "review_text = data.select(text_col).filter(F.col(text_col).isNotNull())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DADT55XOJ8jz"
      },
      "source": [
        "## 2. Define spark pipeline - standard preprocessing (BiGram, NGram, and POSTagging)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EOOz7htuFYSc",
        "outputId": "121b508d-4666-4e50-e4d9-e2844e1d4fda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "lemma_antbnc download started this may take some time.\n",
            "Approximate size to download 907.6 KB\n",
            "[OK!]\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "pos_anc download started this may take some time.\n",
            "Approximate size to download 3.9 MB\n",
            "[OK!]\n"
          ]
        }
      ],
      "source": [
        "from sparknlp.base import DocumentAssembler\n",
        "from sparknlp.annotator import Tokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from sparknlp.annotator import StopWordsCleaner\n",
        "from sparknlp.annotator import NGramGenerator\n",
        "from sparknlp.annotator import PerceptronModel\n",
        "from sparknlp.base import Finisher\n",
        "from pyspark.ml import Pipeline\n",
        "from sparknlp.annotator import Normalizer\n",
        "from sparknlp.annotator import LemmatizerModel\n",
        "import nltk\n",
        "\n",
        "documentAssembler = DocumentAssembler() \\\n",
        "     .setInputCol(text_col) \\\n",
        "     .setOutputCol('document')\n",
        "\n",
        "tokenizer = Tokenizer() \\\n",
        "     .setInputCols(['document']) \\\n",
        "     .setOutputCol('tokenized')\n",
        "\n",
        "normalizer = Normalizer() \\\n",
        "     .setInputCols(['tokenized']) \\\n",
        "     .setOutputCol('normalized') \\\n",
        "     .setLowercase(True)\n",
        "\n",
        "lemmatizer = LemmatizerModel.pretrained() \\\n",
        "     .setInputCols(['normalized']) \\\n",
        "     .setOutputCol('lemmatized')\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "eng_stopwords = stopwords.words('english')\n",
        "\n",
        "stopwords_cleaner = StopWordsCleaner() \\\n",
        "     .setInputCols(['lemmatized']) \\\n",
        "     .setOutputCol('unigrams') \\\n",
        "     .setStopWords(eng_stopwords)\n",
        "\n",
        "ngrammer = NGramGenerator() \\\n",
        "    .setInputCols(['lemmatized']) \\\n",
        "    .setOutputCol('ngrams') \\\n",
        "    .setN(3) \\\n",
        "    .setEnableCumulative(True) \\\n",
        "    .setDelimiter('_')\n",
        "\n",
        "pos_tagger = PerceptronModel.pretrained('pos_anc') \\\n",
        "    .setInputCols(['document', 'lemmatized']) \\\n",
        "    .setOutputCol('pos')\n",
        "\n",
        "finisher = Finisher() \\\n",
        "     .setInputCols(['unigrams', 'ngrams', 'pos']) \\\n",
        "\n",
        "pipeline = Pipeline() \\\n",
        "     .setStages([documentAssembler,                  \n",
        "                 tokenizer,\n",
        "                 normalizer,                  \n",
        "                 lemmatizer,                  \n",
        "                 stopwords_cleaner, \n",
        "                 ngrammer,  \n",
        "                 pos_tagger,\n",
        "                 finisher])                  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmJs85ONQlL8",
        "outputId": "27998a46-25f4-4c37-ede2-4c44f822659d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "|                text|   finished_unigrams|     finished_ngrams|        finished_pos|\n",
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "|procedure total h...|[procedure, total...|[procedure, total...|[NN, JJ, NN, NN, ...|\n",
            "|procedure perform...|[procedure, perfo...|[procedure, perfo...|[NN, NN, NN, NN, ...|\n",
            "|preop diagnoses 1...|[preop, diagnosis...|[preop, diagnosis...|[NN, NN, NN, NN, ...|\n",
            "|preoperative diag...|[preoperative, di...|[preoperative, di...|[JJ, NN, NN, NN, ...|\n",
            "|preoperative diag...|[preoperative, di...|[preoperative, di...|[JJ, NN, JJ, NN, ...|\n",
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "processed_review = pipeline.fit(review_text).transform(review_text)\n",
        "processed_review.limit(5).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MC1PnciBYaoE",
        "outputId": "f03da9b2-a74b-423b-d65c-137e384a8bf8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "|                text|   finished_unigrams|     finished_ngrams|        finished_pos|\n",
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "|procedure total h...|[procedure, total...|[procedure, total...|NN JJ NN NN NN NN...|\n",
            "|procedure perform...|[procedure, perfo...|[procedure, perfo...|NN NN NN NN IN NN...|\n",
            "|preop diagnoses 1...|[preop, diagnosis...|[preop, diagnosis...|NN NN NN NN NN NN...|\n",
            "|preoperative diag...|[preoperative, di...|[preoperative, di...|JJ NN NN NN NN JJ...|\n",
            "|preoperative diag...|[preoperative, di...|[preoperative, di...|JJ NN JJ NN NN IN...|\n",
            "|the patient was c...|[patient, consent...|[the, patient, be...|DT NN VB NN IN NN...|\n",
            "|preoperative diag...|[preoperative, di...|[preoperative, di...|JJ NN NN NN JJ NN...|\n",
            "|preoperative diag...|[preoperative, di...|[preoperative, di...|JJ NN NN NN NN IN...|\n",
            "|preoperative diag...|[preoperative, di...|[preoperative, di...|JJ NN NN NN NN JJ...|\n",
            "|admitting diagnos...|[admit, diagnosis...|[admit, diagnosis...|NN NN NN NN NN NN...|\n",
            "|reason for consul...|[reason, consulta...|[reason, for, con...|NN IN NN NN NN IN...|\n",
            "|reason for referr...|[reason, referral...|[reason, for, ref...|NN IN NN DT NN VB...|\n",
            "|history of presen...|[history, present...|[history, of, pre...|NN IN JJ NN DT VB...|\n",
            "|cc slowing of mot...|[cc, slow, motor,...|[cc, slow, of, mo...|NN JJ IN NN NN CC...|\n",
            "|history of presen...|[history, present...|[history, of, pre...|NN IN JJ NN DT NN...|\n",
            "|history neurologi...|[history, neurolo...|[history, neurolo...|NN JJ NN VB NN TO...|\n",
            "|type of consultat...|[type, consultati...|[type, of, consul...|NN IN NN NN NN NN...|\n",
            "|the patient is a ...|[patient, yearold...|[the, patient, be...|DT NN VB DT JJ NN...|\n",
            "|chief complaint p...|[chief, complaint...|[chief, complaint...|JJ NN NN NN NN JJ...|\n",
            "|reason for referr...|[reason, referral...|[reason, for, ref...|NN IN NN DT NN VB...|\n",
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import types as T\n",
        "\n",
        "udf_join_arr = F.udf(lambda x: ' '.join(x), T.StringType())\n",
        "processed_review  = processed_review.withColumn('finished_pos', udf_join_arr(F.col('finished_pos')))\n",
        "processed_review.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ly75bryuYb7C"
      },
      "outputs": [],
      "source": [
        "pos_documentAssembler = DocumentAssembler() \\\n",
        "     .setInputCol('finished_pos') \\\n",
        "     .setOutputCol('pos_document')\n",
        "\n",
        "pos_tokenizer = Tokenizer() \\\n",
        "     .setInputCols(['pos_document']) \\\n",
        "     .setOutputCol('pos')\n",
        "\n",
        "pos_ngrammer = NGramGenerator() \\\n",
        "    .setInputCols(['pos']) \\\n",
        "    .setOutputCol('pos_ngrams') \\\n",
        "    .setN(3) \\\n",
        "    .setEnableCumulative(True) \\\n",
        "    .setDelimiter('_')\n",
        "\n",
        "pos_finisher = Finisher() \\\n",
        "     .setInputCols(['pos', 'pos_ngrams']) \\\n",
        "\n",
        "pos_pipeline = Pipeline().setStages([pos_documentAssembler, pos_tokenizer, pos_ngrammer, pos_finisher])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z58nWobsYc6u",
        "outputId": "7722a8c4-2c6e-4512-d9b8-90af34adb90c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|                text|   finished_unigrams|     finished_ngrams|        finished_pos| finished_pos_ngrams|\n",
            "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|procedure total h...|[procedure, total...|[procedure, total...|[NN, JJ, NN, NN, ...|[NN, JJ, NN, NN, ...|\n",
            "|procedure perform...|[procedure, perfo...|[procedure, perfo...|[NN, NN, NN, NN, ...|[NN, NN, NN, NN, ...|\n",
            "|preop diagnoses 1...|[preop, diagnosis...|[preop, diagnosis...|[NN, NN, NN, NN, ...|[NN, NN, NN, NN, ...|\n",
            "|preoperative diag...|[preoperative, di...|[preoperative, di...|[JJ, NN, NN, NN, ...|[JJ, NN, NN, NN, ...|\n",
            "|preoperative diag...|[preoperative, di...|[preoperative, di...|[JJ, NN, JJ, NN, ...|[JJ, NN, JJ, NN, ...|\n",
            "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "processed_review = pos_pipeline.fit(processed_review).transform(processed_review)\n",
        "processed_review.limit(5).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4NILXEZhrAQ"
      },
      "source": [
        "#### Filtering data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sowTzoKrYeBn"
      },
      "outputs": [],
      "source": [
        "def filter_pos(words, pos_tags):\n",
        "    return [word for word, pos in zip(words, pos_tags) \n",
        "            if pos in ['JJ', 'NN', 'NNS', 'VB', 'VBP']]\n",
        "\n",
        "udf_filter_pos = F.udf(filter_pos, T.ArrayType(T.StringType()))\n",
        "processed_review = processed_review.withColumn('filtered_unigrams',\n",
        "                                               udf_filter_pos(F.col('finished_unigrams'), \n",
        "                                                              F.col('finished_pos')))\n",
        "def filter_pos_combs(words, pos_tags):\n",
        "    return [word for word, pos in zip(words, pos_tags) \n",
        "            if (len(pos.split('_')) == 2 and \\\n",
        "                pos.split('_')[0] in ['JJ', 'NN', 'NNS', 'VB', 'VBP'] and \\\n",
        "                 pos.split('_')[1] in ['JJ', 'NN', 'NNS']) \\\n",
        "            or (len(pos.split('_')) == 3 and \\\n",
        "                pos.split('_')[0] in ['JJ', 'NN', 'NNS', 'VB', 'VBP'] and \\\n",
        "                 pos.split('_')[1] in ['JJ', 'NN', 'NNS', 'VB', 'VBP'] and \\\n",
        "                  pos.split('_')[2] in ['NN', 'NNS'])]\n",
        "    \n",
        "udf_filter_pos_combs = F.udf(filter_pos_combs, T.ArrayType(T.StringType()))\n",
        "processed_review = processed_review.withColumn('filtered_ngrams',\n",
        "                                               udf_filter_pos_combs(F.col('finished_ngrams'),\n",
        "                                                                    F.col('finished_pos_ngrams')))\n",
        "from pyspark.sql.functions import concat\n",
        "processed_review = processed_review.withColumn('final', \n",
        "                                               concat(F.col('filtered_unigrams'), \n",
        "                                                      F.col('filtered_ngrams')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1iumgBm1vHR",
        "outputId": "d8a04870-18af-435e-9adf-c927f9e8ff78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|                text|   finished_unigrams|     finished_ngrams|        finished_pos| finished_pos_ngrams|   filtered_unigrams|     filtered_ngrams|               final|\n",
            "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|procedure total h...|[procedure, total...|[procedure, total...|[NN, JJ, NN, NN, ...|[NN, JJ, NN, NN, ...|[procedure, total...|[procedure_total,...|[procedure, total...|\n",
            "|procedure perform...|[procedure, perfo...|[procedure, perfo...|[NN, NN, NN, NN, ...|[NN, NN, NN, NN, ...|[procedure, perfo...|[procedure_perfor...|[procedure, perfo...|\n",
            "|preop diagnoses 1...|[preop, diagnosis...|[preop, diagnosis...|[NN, NN, NN, NN, ...|[NN, NN, NN, NN, ...|[preop, diagnosis...|[preop_diagnosis,...|[preop, diagnosis...|\n",
            "|preoperative diag...|[preoperative, di...|[preoperative, di...|[JJ, NN, NN, NN, ...|[JJ, NN, NN, NN, ...|[preoperative, di...|[preoperative_dia...|[preoperative, di...|\n",
            "|preoperative diag...|[preoperative, di...|[preoperative, di...|[JJ, NN, JJ, NN, ...|[JJ, NN, JJ, NN, ...|[preoperative, di...|[preoperative_dia...|[preoperative, di...|\n",
            "|the patient was c...|[patient, consent...|[the, patient, be...|[DT, NN, VB, NN, ...|[DT, NN, VB, NN, ...|[consent, skin, b...|[be_consent, skin...|[consent, skin, b...|\n",
            "|preoperative diag...|[preoperative, di...|[preoperative, di...|[JJ, NN, NN, NN, ...|[JJ, NN, NN, NN, ...|[preoperative, di...|[preoperative_dia...|[preoperative, di...|\n",
            "|preoperative diag...|[preoperative, di...|[preoperative, di...|[JJ, NN, NN, NN, ...|[JJ, NN, NN, NN, ...|[preoperative, di...|[preoperative_dia...|[preoperative, di...|\n",
            "|preoperative diag...|[preoperative, di...|[preoperative, di...|[JJ, NN, NN, NN, ...|[JJ, NN, NN, NN, ...|[preoperative, di...|[preoperative_dia...|[preoperative, di...|\n",
            "|admitting diagnos...|[admit, diagnosis...|[admit, diagnosis...|[NN, NN, NN, NN, ...|[NN, NN, NN, NN, ...|[admit, diagnosis...|[admit_diagnosis,...|[admit, diagnosis...|\n",
            "|reason for consul...|[reason, consulta...|[reason, for, con...|[NN, IN, NN, NN, ...|[NN, IN, NN, NN, ...|[reason, mesothel...|[consultation_mes...|[reason, mesothel...|\n",
            "|reason for referr...|[reason, referral...|[reason, for, ref...|[NN, IN, NN, DT, ...|[NN, IN, NN, DT, ...|[reason, patient,...|[hospitalist_serv...|[reason, patient,...|\n",
            "|history of presen...|[history, present...|[history, of, pre...|[NN, IN, JJ, NN, ...|[NN, IN, JJ, NN, ...|[history, illness...|[present_illness,...|[history, illness...|\n",
            "|cc slowing of mot...|[cc, slow, motor,...|[cc, slow, of, mo...|[NN, JJ, IN, NN, ...|[NN, JJ, IN, NN, ...|[cc, slow, skill,...|[cc_slow, motor_s...|[cc, slow, skill,...|\n",
            "|history of presen...|[history, present...|[history, of, pre...|[NN, IN, JJ, NN, ...|[NN, IN, JJ, NN, ...|[history, illness...|[present_illness,...|[history, illness...|\n",
            "|history neurologi...|[history, neurolo...|[history, neurolo...|[NN, JJ, NN, VB, ...|[NN, JJ, NN, VB, ...|[history, neurolo...|[history_neurolog...|[history, neurolo...|\n",
            "|type of consultat...|[type, consultati...|[type, of, consul...|[NN, IN, NN, NN, ...|[NN, IN, NN, NN, ...|[type, wind, care...|[consultation_win...|[type, wind, care...|\n",
            "|the patient is a ...|[patient, yearold...|[the, patient, be...|[DT, NN, VB, DT, ...|[DT, NN, VB, DT, ...|[yearold, female,...|[yearold_female, ...|[yearold, female,...|\n",
            "|chief complaint p...|[chief, complaint...|[chief, complaint...|[JJ, NN, NN, NN, ...|[JJ, NN, NN, NN, ...|[chief, complaint...|[chief_complaint,...|[chief, complaint...|\n",
            "|reason for referr...|[reason, referral...|[reason, for, ref...|[NN, IN, NN, DT, ...|[NN, IN, NN, DT, ...|[reason, patient,...|[yearold_caucasia...|[reason, patient,...|\n",
            "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "processed_review.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULTWYu08Zfg-"
      },
      "source": [
        "### Vectorizer and modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZysVwaKZfmu"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import CountVectorizer\n",
        "\n",
        "tfizer = CountVectorizer(inputCol='final', outputCol='tf_features')\n",
        "tf_model = tfizer.fit(processed_review)\n",
        "tf_result = tf_model.transform(processed_review)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-xToiaOZgz-"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import IDF\n",
        "\n",
        "idfizer = IDF(inputCol='tf_features', outputCol='tf_idf_features')\n",
        "idf_model = idfizer.fit(tf_result)\n",
        "tfidf_result = idf_model.transform(tf_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-b2oL6NZh-J"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.clustering import LDA\n",
        "\n",
        "num_topics = 25\n",
        "max_iter = 10\n",
        "\n",
        "lda = LDA(k=num_topics, maxIter=max_iter, featuresCol='tf_idf_features')\n",
        "lda_model = lda.fit(tfidf_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aPDO_9nzZjSW"
      },
      "outputs": [],
      "source": [
        "vocab = tf_model.vocabulary\n",
        "\n",
        "def get_words(token_list):\n",
        "     return [vocab[token_id] for token_id in token_list]\n",
        "       \n",
        "udf_to_words = F.udf(get_words, T.ArrayType(T.StringType()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avpzxjJAZksa",
        "outputId": "defa67ca-ef24-431e-b19e-14d5ea3954dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+------------------------------------------------------------------------------------------+\n",
            "|topic|                                                                                topicWords|\n",
            "+-----+------------------------------------------------------------------------------------------+\n",
            "|    0|[leave_mandibular, mandibular_dental_abscess, mandibular_dental, extraction, carious_to...|\n",
            "|    1|[preservative, preservative_free, record_complication, epidural, be_inject_neosporin, b...|\n",
            "|    2|[mmddyyyy, renal, renal_cyst, dr_xyz, cell_carcinoma, leave_renal, cyst, carcinoma, che...|\n",
            "|    3|[endoscopy_suite, os, endoscopy, mass_lesion, suite, sims_position, abnormality_friabil...|\n",
            "|    4|[tumor, place, fracture, procedure, right_eye, anesthesia, be_place, incision, remove, ...|\n",
            "|    5|[average_range, premorbid, learn, repetition, high_average_range, task, reportedly, sec...|\n",
            "|    6|[vulgaris_plan_recommend, abnormal_menstrual, good_backflow, discharge_discharge_diagno...|\n",
            "|    7|[disk, artificial_disk, l, great_toe, screw, upper_extremity, low_extremity, fusion, xr...|\n",
            "|    8|[carbohydrate, meal, twohour_postprandial, bedtime_snack, meal_plan, sugar, calory, las...|\n",
            "|    9|[obvious, be_normal, evidence, carotid_artery, normal, palpable, note, carotid, breast,...|\n",
            "|   10|[gastroesophageal, reflux, gastroesophageal_junction, gastroesophageal_reflux, hiatal_h...|\n",
            "|   11|[os, mgdl, temporal, bilateral_babinski, leave_eye, sellar, cn_palsy, sellar_enlargemen...|\n",
            "|   12|[total_knee, knee, knee_replacement, total_knee_replacement, replacement_cataract_enlar...|\n",
            "|   13|              [mg_po, mg, po, daily, po_daily, discharge, history, disorder, insulin, day]|\n",
            "|   14|[fracture, mandible, angle, reduction, internal_fixation, open_reduction, fixation, lea...|\n",
            "|   15|          [os, cn, local_hospital, emergent, od, aneurysm, expressive, brain, mgdl, hr_rr]|\n",
            "|   16|[uterus, uterus_hypermenorrhea, abnormal_uterine_bleed, hypermenorrhea, bleed_enlarge, ...|\n",
            "|   17|[cervical, cervical_epidural, cervical_fusion, steroid_injection, cervical_epidural_ste...|\n",
            "|   18|[mandible, low_lateral_cartilage, fracture, conchal, angle, conchal_cartilage, lateral_...|\n",
            "|   19|[mg_po, insulin_pump, po, insulin_pump_rate, pump_rate, mg, anemia, insulin, central_ca...|\n",
            "+-----+------------------------------------------------------------------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "num_top_words = 10\n",
        "\n",
        "topics = lda_model.describeTopics(num_top_words).withColumn('topicWords', udf_to_words(F.col('termIndices')))\n",
        "topics.select('topic', 'topicWords').show(truncate=90)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iu4_Ac05BFBv"
      },
      "outputs": [],
      "source": [
        "df_topik = topics.select('topic', 'topicWords').toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KYeQtAeADayu"
      },
      "outputs": [],
      "source": [
        "df_topik.to_csv('topic.csv',index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bFN6Hb6hAzkm"
      },
      "outputs": [],
      "source": [
        "df.to_csv('inputdata.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQuP6z_0Lz--"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as NLP\n",
        "\n",
        "topic = pd.read_csv('topic.csv')\n",
        "inputdata = pd.read_csv('inputdata.csv')\n",
        "\n",
        "def max_distribution(s):\n",
        "    return topic.topicWords.apply(lambda x: x.replace('[', '').replace(']', '').replace(',', '').replace(\"'\", '').split(' ')).apply(lambda x: sum([1 for p in x if p in s]) / len(x)).idxmax()\n",
        "def percentage(s):\n",
        "    return topic.topicWords.apply(lambda x: x.replace('[', '').replace(']', '').replace(',', '').replace(\"'\", '').split(' ')).apply(lambda x: sum([1 for p in x if p in s]) / len(x)).max()\n",
        "\n",
        "inputdata['topic'] = inputdata.text.apply(lambda x: max_distribution(x))\n",
        "inputdata['percentage'] = inputdata.text.apply(lambda x: percentage(x))\n",
        "\n",
        "inputdata[['medical_speciality', 'topic']].groupby('medical_speciality').agg(list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0omKOuNDFJa"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RgiqfX5XDqb"
      },
      "source": [
        "## 3. Select the NER model and construct the pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6MDlWmDuaLq"
      },
      "source": [
        "Select the NER model - Lab Results models: **ner_jsl, ner_jsl_enriched**\n",
        "\n",
        "For more details: https://github.com/JohnSnowLabs/spark-nlp-models#pretrained-models---spark-nlp-for-healthcare"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "He3s9CAvucfe"
      },
      "outputs": [],
      "source": [
        "# You can change this to the model you want to use and re-run cells below.\n",
        "# Lab models: ner_jsl, ner_jsl_enriched\n",
        "MODEL_NAME = \"ner_events_clinical\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zweiG2ilZqoR"
      },
      "source": [
        "Create the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLuDz_t40be4",
        "outputId": "e6e1dc23-5a61-42f2-ad2c-41781652ff95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "embeddings_clinical download started this may take some time.\n",
            "Approximate size to download 1.6 GB\n",
            "[OK!]\n",
            "ner_events_clinical download started this may take some time.\n",
            "Approximate size to download 13.8 MB\n",
            "[OK!]\n"
          ]
        }
      ],
      "source": [
        "document_assembler = DocumentAssembler() \\\n",
        "    .setInputCol('text')\\\n",
        "    .setOutputCol('document')\n",
        "\n",
        "sentence_detector = SentenceDetector() \\\n",
        "    .setInputCols(['document'])\\\n",
        "    .setOutputCol('sentence')\n",
        "\n",
        "tokenizer = Tokenizer()\\\n",
        "    .setInputCols(['sentence']) \\\n",
        "    .setOutputCol('token')\n",
        "\n",
        "word_embeddings = WordEmbeddingsModel.pretrained('embeddings_clinical', 'en', 'clinical/models') \\\n",
        "    .setInputCols(['document', 'token']) \\\n",
        "    .setOutputCol('embeddings')\n",
        "\n",
        "clinical_ner = MedicalNerModel.pretrained(MODEL_NAME, \"en\", \"clinical/models\") \\\n",
        "          .setInputCols([\"document\", \"token\", \"embeddings\"]) \\\n",
        "          .setOutputCol(\"ner\")\n",
        "\n",
        "ner_converter = NerConverter()\\\n",
        "    .setInputCols(['sentence', 'token', 'ner']) \\\n",
        "    .setOutputCol('ner_chunk')\n",
        "\n",
        "nlp_pipeline = Pipeline(stages=[\n",
        "    document_assembler, \n",
        "    sentence_detector,\n",
        "    tokenizer,\n",
        "    word_embeddings,\n",
        "    clinical_ner,\n",
        "    ner_converter])\n",
        "\n",
        "empty_df = spark.createDataFrame([['']]).toDF(\"text\")\n",
        "\n",
        "pipelineModel = nlp_pipeline.fit(empty_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCN_n5gFe1tc"
      },
      "source": [
        "`MedicalNerModel is limited session`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZsEeNRHCRdNq",
        "outputId": "a4b9e552-aa38-4910-e35b-84894cb641b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------------------------------------------+-------------+\n",
            "|chunk                                                |ner_label    |\n",
            "+-----------------------------------------------------+-------------+\n",
            "|procedure total hip replacement  procedure           |TREATMENT    |\n",
            "|the operating room                                   |CLINICAL_DEPT|\n",
            "|anesthesia                                           |TREATMENT    |\n",
            "|an incision                                          |TREATMENT    |\n",
            "|the greater trochanter dissection                    |TREATMENT    |\n",
            "|incised                                              |TREATMENT    |\n",
            "|split proximally the piriformis and external rotators|TREATMENT    |\n",
            "|their insertions on the greater trochanter           |TREATMENT    |\n",
            "|a sleeve                                             |TREATMENT    |\n",
            "|a femoral neck cut                                   |TEST         |\n",
            "|preoperative templating the femoral head             |TEST         |\n",
            "|extensive degenerative disease                       |PROBLEM      |\n",
            "|the acetabulum  baseline leg-length measurements     |TEST         |\n",
            "|retracted anteriorly                                 |TREATMENT    |\n",
            "|a complete labrectomy                                |TREATMENT    |\n",
            "|reaming of the acetabulum                            |TREATMENT    |\n",
            "|adequate bleeding subchondral bone                   |PROBLEM      |\n",
            "|the trial shell                                      |TREATMENT    |\n",
            "|anteversion and abduction screws                     |TREATMENT    |\n",
            "|drilling into the pelvis measuring                   |TREATMENT    |\n",
            "+-----------------------------------------------------+-------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "result = pipelineModel.transform(review_text.limit(10))\n",
        "\n",
        "result.select(F.explode(F.arrays_zip('ner_chunk.result', 'ner_chunk.metadata')).alias(\"cols\")) \\\n",
        ".select(F.expr(\"cols['0']\").alias(\"chunk\"),\n",
        "        F.expr(\"cols['1']['entity']\").alias(\"ner_label\")).show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3UD9agDcPbs"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import types as T\n",
        "\n",
        "processed_review = pipelineModel.transform(review_text)\n",
        "\n",
        "processed_review  = processed_review.withColumn('final', F.col('ner_chunk.result'))\n",
        "# processed_review.limit(2).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8AFytnujmGuF",
        "outputId": "9a8826e7-48b6-4f5d-b483-635a52b6f7db"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "    @import url('https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400;500;600;700&display=swap');\n",
              "    @import url('https://fonts.googleapis.com/css2?family=Vistol Regular:wght@300;400;500;600;700&display=swap');\n",
              "    \n",
              "    .spark-nlp-display-scroll-entities {\n",
              "        border: 1px solid #E7EDF0;\n",
              "        border-radius: 3px;\n",
              "        text-align: justify;\n",
              "        \n",
              "    }\n",
              "    .spark-nlp-display-scroll-entities span {  \n",
              "        font-size: 14px;\n",
              "        line-height: 24px;\n",
              "        color: #536B76;\n",
              "        font-family: 'Montserrat', sans-serif !important;\n",
              "    }\n",
              "    \n",
              "    .spark-nlp-display-entity-wrapper{\n",
              "    \n",
              "        display: inline-grid;\n",
              "        text-align: center;\n",
              "        border-radius: 4px;\n",
              "        margin: 0 2px 5px 2px;\n",
              "        padding: 1px\n",
              "    }\n",
              "    .spark-nlp-display-entity-name{\n",
              "        font-size: 14px;\n",
              "        line-height: 24px;\n",
              "        font-family: 'Montserrat', sans-serif !important;\n",
              "        \n",
              "        background: #f1f2f3;\n",
              "        border-width: medium;\n",
              "        text-align: center;\n",
              "        \n",
              "        font-weight: 400;\n",
              "        \n",
              "        border-radius: 5px;\n",
              "        padding: 2px 5px;\n",
              "        display: block;\n",
              "        margin: 3px 2px;\n",
              "    \n",
              "    }\n",
              "    .spark-nlp-display-entity-type{\n",
              "        font-size: 14px;\n",
              "        line-height: 24px;\n",
              "        color: #ffffff;\n",
              "        font-family: 'Montserrat', sans-serif !important;\n",
              "        \n",
              "        text-transform: uppercase;\n",
              "        \n",
              "        font-weight: 500;\n",
              "\n",
              "        display: block;\n",
              "        padding: 3px 5px;\n",
              "    }\n",
              "    \n",
              "    .spark-nlp-display-entity-resolution{\n",
              "        font-size: 14px;\n",
              "        line-height: 24px;\n",
              "        color: #ffffff;\n",
              "        font-family: 'Vistol Regular', sans-serif !important;\n",
              "        \n",
              "        text-transform: uppercase;\n",
              "        \n",
              "        font-weight: 500;\n",
              "\n",
              "        display: block;\n",
              "        padding: 3px 5px;\n",
              "    }\n",
              "    \n",
              "    .spark-nlp-display-others{\n",
              "        font-size: 14px;\n",
              "        line-height: 24px;\n",
              "        font-family: 'Montserrat', sans-serif !important;\n",
              "        \n",
              "        font-weight: 400;\n",
              "    }\n",
              "\n",
              "</style>\n",
              " <span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #8b6673\"><span class=\"spark-nlp-display-entity-name\">procedure total hip replacement  procedure </span><span class=\"spark-nlp-display-entity-type\">TREATMENT</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> description the patient was bought to </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #240117\"><span class=\"spark-nlp-display-entity-name\">the operating room </span><span class=\"spark-nlp-display-entity-type\">CLINICAL_DEPT</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> and placed in the supine position after induction of </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #8b6673\"><span class=\"spark-nlp-display-entity-name\">anesthesia </span><span class=\"spark-nlp-display-entity-type\">TREATMENT</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> the patient was turned on the side and secured in the hip table </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #8b6673\"><span class=\"spark-nlp-display-entity-name\">an incision </span><span class=\"spark-nlp-display-entity-type\">TREATMENT</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> was made centered over </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #8b6673\"><span class=\"spark-nlp-display-entity-name\">the greater trochanter dissection </span><span class=\"spark-nlp-display-entity-type\">TREATMENT</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> was sharply carried down through the subcutaneous tissues the gluteus maximus was </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #8b6673\"><span class=\"spark-nlp-display-entity-name\">incised </span><span class=\"spark-nlp-display-entity-type\">TREATMENT</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> and </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #8b6673\"><span class=\"spark-nlp-display-entity-name\">split proximally the piriformis and external rotators </span><span class=\"spark-nlp-display-entity-type\">TREATMENT</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> were identified these were removed from </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #8b6673\"><span class=\"spark-nlp-display-entity-name\">their insertions on the greater trochanter </span><span class=\"spark-nlp-display-entity-type\">TREATMENT</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> as </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #8b6673\"><span class=\"spark-nlp-display-entity-name\">a sleeve </span><span class=\"spark-nlp-display-entity-type\">TREATMENT</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> with the hip capsule the hip was dislocated </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #77b5fe\"><span class=\"spark-nlp-display-entity-name\">a femoral neck cut </span><span class=\"spark-nlp-display-entity-type\">TEST</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> was made using the guidance of </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #77b5fe\"><span class=\"spark-nlp-display-entity-name\">preoperative templating the femoral head </span><span class=\"spark-nlp-display-entity-type\">TEST</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> was removed </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #800080\"><span class=\"spark-nlp-display-entity-name\">extensive degenerative disease </span><span class=\"spark-nlp-display-entity-type\">PROBLEM</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> was found on the femoral head as well as in </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #77b5fe\"><span class=\"spark-nlp-display-entity-name\">the acetabulum  baseline leg-length measurements </span><span class=\"spark-nlp-display-entity-type\">TEST</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> were taken the femur was </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #8b6673\"><span class=\"spark-nlp-display-entity-name\">retracted anteriorly </span><span class=\"spark-nlp-display-entity-type\">TREATMENT</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> and </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #8b6673\"><span class=\"spark-nlp-display-entity-name\">a complete labrectomy </span><span class=\"spark-nlp-display-entity-type\">TREATMENT</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> was performed </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #8b6673\"><span class=\"spark-nlp-display-entity-name\">reaming of the acetabulum </span><span class=\"spark-nlp-display-entity-type\">TREATMENT</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> was then performed until </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #800080\"><span class=\"spark-nlp-display-entity-name\">adequate bleeding subchondral bone </span><span class=\"spark-nlp-display-entity-type\">PROBLEM</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> was identified in the key areas </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #8b6673\"><span class=\"spark-nlp-display-entity-name\">the trial shell </span><span class=\"spark-nlp-display-entity-type\">TREATMENT</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> was placed and found to have an excellent fit the real shell was opened and impacted into position in the appropriate amount of </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #8b6673\"><span class=\"spark-nlp-display-entity-name\">anteversion and abduction screws </span><span class=\"spark-nlp-display-entity-type\">TREATMENT</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> were placed by </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #8b6673\"><span class=\"spark-nlp-display-entity-name\">drilling into the pelvis measuring </span><span class=\"spark-nlp-display-entity-type\">TREATMENT</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> and placing </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #8b6673\"><span class=\"spark-nlp-display-entity-name\">the appropriate length screw </span><span class=\"spark-nlp-display-entity-type\">TREATMENT</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> excellent purchase was obtained </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #8b6673\"><span class=\"spark-nlp-display-entity-name\">the trial liner </span><span class=\"spark-nlp-display-entity-type\">TREATMENT</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> was placed  </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #8b6673\"><span class=\"spark-nlp-display-entity-name\">the femur was then flexed </span><span class=\"spark-nlp-display-entity-type\">TREATMENT</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> and internally rotated </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #8b6673\"><span class=\"spark-nlp-display-entity-name\">the extra trochanteric bone </span><span class=\"spark-nlp-display-entity-type\">TREATMENT</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> was removed as was any leftover </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #8b6673\"><span class=\"spark-nlp-display-entity-name\">lateral soft tissue at the piriformis insertion </span><span class=\"spark-nlp-display-entity-type\">TREATMENT</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> an intramedullary hole was drilled into the femur to define </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #8b6673\"><span class=\"spark-nlp-display-entity-name\">the canal reaming </span><span class=\"spark-nlp-display-entity-type\">TREATMENT</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> was performed until the appropriate size was reached </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #8b6673\"><span class=\"spark-nlp-display-entity-name\">the broaches </span><span class=\"spark-nlp-display-entity-type\">TREATMENT</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> were then used to prepare </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #8b6673\"><span class=\"spark-nlp-display-entity-name\">the femur </span><span class=\"spark-nlp-display-entity-type\">TREATMENT</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> with the appropriate amount of version once </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #8b6673\"><span class=\"spark-nlp-display-entity-name\">the appropriate size broach </span><span class=\"spark-nlp-display-entity-type\">TREATMENT</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> was reached it was used as </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #8b6673\"><span class=\"spark-nlp-display-entity-name\">a trial </span><span class=\"spark-nlp-display-entity-type\">TREATMENT</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> with </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #8b6673\"><span class=\"spark-nlp-display-entity-name\">head and neck placement hip range-of-motion </span><span class=\"spark-nlp-display-entity-type\">TREATMENT</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> was checked in all planes including flexion-internal rotation the position of sleep and extension-external rotation the hip was found to have excellent stability with the final chosen </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #77b5fe\"><span class=\"spark-nlp-display-entity-name\">head-neck combination leg length measurements </span><span class=\"spark-nlp-display-entity-type\">TEST</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> were taken and found to be within acceptable range given the necessity for </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #8b6673\"><span class=\"spark-nlp-display-entity-name\">stability  the real stem </span><span class=\"spark-nlp-display-entity-type\">TREATMENT</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> was opened and impacted into position the real head was impacted atop the stem if </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #8b6673\"><span class=\"spark-nlp-display-entity-name\">cement </span><span class=\"spark-nlp-display-entity-type\">TREATMENT</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> was used the canal was thoroughly washed and </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #8b6673\"><span class=\"spark-nlp-display-entity-name\">dried </span><span class=\"spark-nlp-display-entity-type\">TREATMENT</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> and </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #8b6673\"><span class=\"spark-nlp-display-entity-name\">plugged </span><span class=\"spark-nlp-display-entity-type\">TREATMENT</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> with </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #8b6673\"><span class=\"spark-nlp-display-entity-name\">a restrictor </span><span class=\"spark-nlp-display-entity-type\">TREATMENT</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> and then </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #8b6673\"><span class=\"spark-nlp-display-entity-name\">the cement </span><span class=\"spark-nlp-display-entity-type\">TREATMENT</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> was injected and </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #8b6673\"><span class=\"spark-nlp-display-entity-name\">pressurized </span><span class=\"spark-nlp-display-entity-type\">TREATMENT</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> and </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #8b6673\"><span class=\"spark-nlp-display-entity-name\">the stem </span><span class=\"spark-nlp-display-entity-type\">TREATMENT</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> was implanted in </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #8b6673\"><span class=\"spark-nlp-display-entity-name\">the appropriate version excess cement </span><span class=\"spark-nlp-display-entity-type\">TREATMENT</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> was removed from the edges of the component range of motion and stability were once again checked and found to be excellent adequate hemostasis was obtained </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #8b6673\"><span class=\"spark-nlp-display-entity-name\">vigorous power irrigation </span><span class=\"spark-nlp-display-entity-type\">TREATMENT</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> was used to remove </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #800080\"><span class=\"spark-nlp-display-entity-name\">all debris from the joint </span><span class=\"spark-nlp-display-entity-type\">PROBLEM</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> prior to final reduction  </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #8b6673\"><span class=\"spark-nlp-display-entity-name\">the arthrotomy </span><span class=\"spark-nlp-display-entity-type\">TREATMENT</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> and </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #8b6673\"><span class=\"spark-nlp-display-entity-name\">rotators </span><span class=\"spark-nlp-display-entity-type\">TREATMENT</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> were closed using 1 ethibond through </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #8b6673\"><span class=\"spark-nlp-display-entity-name\">drill holes in the bone </span><span class=\"spark-nlp-display-entity-type\">TREATMENT</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> recreating the posterior hip structural anatomy the gluteus maximus was </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #8b6673\"><span class=\"spark-nlp-display-entity-name\">repaired </span><span class=\"spark-nlp-display-entity-type\">TREATMENT</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> using </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #8b6673\"><span class=\"spark-nlp-display-entity-name\">0 ethibond </span><span class=\"spark-nlp-display-entity-type\">TREATMENT</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> and </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #8b6673\"><span class=\"spark-nlp-display-entity-name\">0 vicryl </span><span class=\"spark-nlp-display-entity-type\">TREATMENT</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #8b6673\"><span class=\"spark-nlp-display-entity-name\">the subcutaneous tissues </span><span class=\"spark-nlp-display-entity-type\">TREATMENT</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> were closed after </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #8b6673\"><span class=\"spark-nlp-display-entity-name\">further irrigation </span><span class=\"spark-nlp-display-entity-type\">TREATMENT</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> with </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #8b6673\"><span class=\"spark-nlp-display-entity-name\">2-0 vicryl </span><span class=\"spark-nlp-display-entity-type\">TREATMENT</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> and </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #8b6673\"><span class=\"spark-nlp-display-entity-name\">monocryl sutures </span><span class=\"spark-nlp-display-entity-type\">TREATMENT</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> the skin was closed with </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #8b6673\"><span class=\"spark-nlp-display-entity-name\">nylon xeroform </span><span class=\"spark-nlp-display-entity-type\">TREATMENT</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> and </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #8b6673\"><span class=\"spark-nlp-display-entity-name\">a sterile dressing </span><span class=\"spark-nlp-display-entity-type\">TREATMENT</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> were applied followed by </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #8b6673\"><span class=\"spark-nlp-display-entity-name\">a cold pack </span><span class=\"spark-nlp-display-entity-type\">TREATMENT</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> and </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #8b6673\"><span class=\"spark-nlp-display-entity-name\">ace wrap </span><span class=\"spark-nlp-display-entity-type\">TREATMENT</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> the patient was </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #752E13\"><span class=\"spark-nlp-display-entity-name\">transferred </span><span class=\"spark-nlp-display-entity-type\">OCCURRENCE</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> to </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #240117\"><span class=\"spark-nlp-display-entity-name\">the recovery room </span><span class=\"spark-nlp-display-entity-type\">CLINICAL_DEPT</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> in </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #752E13\"><span class=\"spark-nlp-display-entity-name\">stable condition </span><span class=\"spark-nlp-display-entity-type\">OCCURRENCE</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> having </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #752E13\"><span class=\"spark-nlp-display-entity-name\">tolerated </span><span class=\"spark-nlp-display-entity-type\">OCCURRENCE</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #8b6673\"><span class=\"spark-nlp-display-entity-name\">the procedure </span><span class=\"spark-nlp-display-entity-type\">TREATMENT</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> well</span></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from sparknlp_display import NerVisualizer\n",
        "\n",
        "NerVisualizer().display(\n",
        "    result = result.collect()[0],\n",
        "    label_col = 'ner_chunk',\n",
        "    document_col = 'document'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocNd0kqAMtiw"
      },
      "source": [
        "`Using combination ner`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YCFi7CadM5bQ",
        "outputId": "e735621d-6a82-4d3d-b03a-62ce03f62cd8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "embeddings_clinical download started this may take some time.\n",
            "Approximate size to download 1.6 GB\n",
            "[OK!]\n",
            "ner_jsl download started this may take some time.\n",
            "Approximate size to download 14.5 MB\n",
            "[ | ]\n",
            "An error occurred while calling z:com.johnsnowlabs.nlp.pretrained.InternalsPythonResourceDownloader.downloadModel.\n",
            ": com.johnsnowlabs.license.exceptions.JslInvalidLicenseException: Cannot check-in the license or license already in use.\n",
            "\tat com.johnsnowlabs.license.FloatingLicense.checkFloatingLicense(Platforms.scala:130)\n",
            "\tat com.johnsnowlabs.license.CheckLicense.checkValidEnvironment(CheckLicense.scala:37)\n",
            "\tat com.johnsnowlabs.license.CheckLicense.checkValidEnvironment$(CheckLicense.scala:21)\n",
            "\tat com.johnsnowlabs.nlp.pretrained.InternalsPythonResourceDownloader$.checkValidEnvironment(InternalsPythonResourceDownloader.scala:15)\n",
            "\tat com.johnsnowlabs.nlp.pretrained.InternalsPythonResourceDownloader$.downloadModel(InternalsPythonResourceDownloader.scala:46)\n",
            "\tat com.johnsnowlabs.nlp.pretrained.InternalsPythonResourceDownloader.downloadModel(InternalsPythonResourceDownloader.scala)\n",
            "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
            "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
            "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
            "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
            "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
            "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
            "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
            "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
            "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
            "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "[OK!]\n"
          ]
        },
        {
          "ename": "Py4JJavaError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-6c02a68f5c63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mword_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordEmbeddingsModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'embeddings_clinical'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'en'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'clinical/models'\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0msetInputCols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'document'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'token'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0msetOutputCol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'embeddings'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mclinical_ner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMedicalNerModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"en\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"clinical/models\"\u001b[0m\u001b[0;34m)\u001b[0m               \u001b[0;34m.\u001b[0m\u001b[0msetInputCols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"document\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"token\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"embeddings\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m               \u001b[0;34m.\u001b[0m\u001b[0msetOutputCol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ner\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mner_converter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNerConverter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;34m.\u001b[0m\u001b[0msetInputCols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentence'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'token'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ner'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0msetOutputCol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ner_chunk'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sparknlp_jsl/annotator.py\u001b[0m in \u001b[0;36mpretrained\u001b[0;34m(name, lang, remote_loc)\u001b[0m\n\u001b[1;32m   6178\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0msparknlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretrained\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mResourceDownloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6179\u001b[0m         return ResourceDownloader.downloadModel(MedicalNerModel, name, lang, remote_loc,\n\u001b[0;32m-> 6180\u001b[0;31m                                                 j_dwn='InternalsPythonResourceDownloader')\n\u001b[0m\u001b[1;32m   6181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sparknlp/pretrained.py\u001b[0m in \u001b[0;36mdownloadModel\u001b[0;34m(reader, name, language, remote_loc, j_dwn)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mstop_threads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sparknlp/pretrained.py\u001b[0m in \u001b[0;36mdownloadModel\u001b[0;34m(reader, name, language, remote_loc, j_dwn)\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mt1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m                 \u001b[0mj_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_internal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_DownloadModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremote_loc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj_dwn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sparknlp/internal.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, reader, name, language, remote_loc, validator)\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremote_loc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         super(_DownloadModel, self).__init__(\"com.johnsnowlabs.nlp.pretrained.\" + validator + \".downloadModel\", reader,\n\u001b[0;32m--> 214\u001b[0;31m                                              name, language, remote_loc)\n\u001b[0m\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sparknlp/internal.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, java_obj, *args)\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mExtendedJavaWrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_java_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sparknlp/internal.py\u001b[0m in \u001b[0;36mnew_java_obj\u001b[0;34m(self, java_class, *args)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnew_java_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjava_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_java_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnew_java_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpylist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjava_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_new_java_obj\u001b[0;34m(java_class, *args)\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mjava_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mjava_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_py2java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mjava_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mjava_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:com.johnsnowlabs.nlp.pretrained.InternalsPythonResourceDownloader.downloadModel.\n: com.johnsnowlabs.license.exceptions.JslInvalidLicenseException: Cannot check-in the license or license already in use.\n\tat com.johnsnowlabs.license.FloatingLicense.checkFloatingLicense(Platforms.scala:130)\n\tat com.johnsnowlabs.license.CheckLicense.checkValidEnvironment(CheckLicense.scala:37)\n\tat com.johnsnowlabs.license.CheckLicense.checkValidEnvironment$(CheckLicense.scala:21)\n\tat com.johnsnowlabs.nlp.pretrained.InternalsPythonResourceDownloader$.checkValidEnvironment(InternalsPythonResourceDownloader.scala:15)\n\tat com.johnsnowlabs.nlp.pretrained.InternalsPythonResourceDownloader$.downloadModel(InternalsPythonResourceDownloader.scala:46)\n\tat com.johnsnowlabs.nlp.pretrained.InternalsPythonResourceDownloader.downloadModel(InternalsPythonResourceDownloader.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
          ]
        }
      ],
      "source": [
        "MODEL_NAMEs = [\"ner_jsl\", \"ner_events_clinical\"]\n",
        "PM = []\n",
        "\n",
        "for modelname in MODEL_NAMEs:\n",
        "\n",
        "    document_assembler = DocumentAssembler() \\\n",
        "    .setInputCol('text')\\\n",
        "    .setOutputCol('document')\n",
        "\n",
        "    sentence_detector = SentenceDetector() \\\n",
        "        .setInputCols(['document'])\\\n",
        "        .setOutputCol('sentence')\n",
        "\n",
        "    tokenizer = Tokenizer()\\\n",
        "        .setInputCols(['sentence']) \\\n",
        "        .setOutputCol('token')\n",
        "\n",
        "    word_embeddings = WordEmbeddingsModel.pretrained('embeddings_clinical', 'en', 'clinical/models') \\\n",
        "        .setInputCols(['document', 'token']) \\\n",
        "        .setOutputCol('embeddings')\n",
        "\n",
        "    clinical_ner = MedicalNerModel.pretrained(modelname, \"en\", \"clinical/models\") \\\n",
        "              .setInputCols([\"document\", \"token\", \"embeddings\"]) \\\n",
        "              .setOutputCol(\"ner\")\n",
        "\n",
        "    ner_converter = NerConverter()\\\n",
        "        .setInputCols(['sentence', 'token', 'ner']) \\\n",
        "        .setOutputCol('ner_chunk')\n",
        "\n",
        "    nlp_pipeline = Pipeline(stages=[\n",
        "        document_assembler, \n",
        "        sentence_detector,\n",
        "        tokenizer,\n",
        "        word_embeddings,\n",
        "        clinical_ner,\n",
        "        ner_converter])\n",
        "\n",
        "    empty_df = spark.createDataFrame([['']]).toDF(\"text\")\n",
        "    pipelineModel = nlp_pipeline.fit(empty_df)\n",
        "    processed_review = pipelineModel.transform(review_text)\n",
        "    processed_review  = processed_review.withColumn(modelname, F.col('ner_chunk.result'))\n",
        "    PM += [processed_review.select(modelname)]\n",
        "\n",
        "for pm in PM[:-1]:\n",
        "    processed_review = processed_review.join(pm)\n",
        "\n",
        "from pyspark.sql.functions import concat\n",
        "processed_review = processed_review.withColumn('final', \n",
        "                                                 concat(F.col(MODEL_NAMEs[0]), \n",
        "                                                        F.col(MODEL_NAMEs[1])))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTLL4oMgNILc"
      },
      "outputs": [],
      "source": [
        "processed_review = processed_review.select(['text', 'final'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qinjaUZZarZ"
      },
      "outputs": [],
      "source": [
        "del PM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeIp8xmoIrA3"
      },
      "source": [
        "### Vectorizer and modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "X2ADDuFfIrA4",
        "outputId": "77146c9f-ac5a-43c6-99b4-715f41fa97d4"
      },
      "outputs": [
        {
          "ename": "Py4JJavaError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-461b7d272af3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtfizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'final'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tf_features'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtf_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_review\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtf_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_review\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \"\"\"\n\u001b[1;32m    331\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o749.fit.\n: org.apache.spark.SparkException: Could not execute broadcast in 300 secs. You can increase the timeout for broadcasts via spark.sql.broadcastTimeout or disable broadcast join by setting spark.sql.autoBroadcastJoinThreshold to -1\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:205)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeBroadcast$1(SparkPlan.scala:193)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.executeBroadcast(SparkPlan.scala:189)\n\tat org.apache.spark.sql.execution.joins.BroadcastNestedLoopJoinExec.doExecute(BroadcastNestedLoopJoinExec.scala:350)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:525)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:453)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:452)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:50)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:746)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.DeserializeToObjectExec.doExecute(objects.scala:96)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\n\tat org.apache.spark.sql.Dataset.rdd$lzycompute(Dataset.scala:3241)\n\tat org.apache.spark.sql.Dataset.rdd(Dataset.scala:3239)\n\tat org.apache.spark.ml.feature.CountVectorizer.fit(CountVectorizer.scala:191)\n\tat org.apache.spark.ml.feature.CountVectorizer.fit(CountVectorizer.scala:149)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.util.concurrent.TimeoutException\n\tat java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:194)\n\t... 45 more\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import CountVectorizer\n",
        "\n",
        "tfizer = CountVectorizer(inputCol='final', outputCol='tf_features')\n",
        "tf_model = tfizer.fit(processed_review)\n",
        "tf_result = tf_model.transform(processed_review)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vAu2h3R4Zn8u"
      },
      "outputs": [],
      "source": [
        "del processed_review"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rco-pZSJIrA4"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import IDF\n",
        "\n",
        "idfizer = IDF(inputCol='tf_features', outputCol='tf_idf_features')\n",
        "idf_model = idfizer.fit(tf_result)\n",
        "tfidf_result = idf_model.transform(tf_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LtmuAXDnIrA4"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.clustering import LDA\n",
        "\n",
        "num_topics = 25\n",
        "max_iter = 10\n",
        "\n",
        "lda = LDA(k=num_topics, maxIter=max_iter, featuresCol='tf_idf_features')\n",
        "lda_model = lda.fit(tfidf_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cE739M3HIrA4"
      },
      "outputs": [],
      "source": [
        "vocab = tf_model.vocabulary\n",
        "\n",
        "def get_words(token_list):\n",
        "     return [vocab[token_id] for token_id in token_list]\n",
        "       \n",
        "udf_to_words = F.udf(get_words, T.ArrayType(T.StringType()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HIOIeJYeIrA4"
      },
      "outputs": [],
      "source": [
        "num_top_words = 10\n",
        "\n",
        "topics = lda_model.describeTopics(num_top_words).withColumn('topicWords', udf_to_words(F.col('termIndices')))\n",
        "topics.select('topic', 'topicWords').show(truncate=90)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ItLJdQZQIrA4"
      },
      "outputs": [],
      "source": [
        "df_topik = topics.select('topic', 'topicWords').toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DaOUpHmgIrA5"
      },
      "outputs": [],
      "source": [
        "df_topik.to_csv('topic_with_ner.csv',index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIU2beRDIrA5"
      },
      "outputs": [],
      "source": [
        "df.to_csv('inputdata_with_ner.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8HM1YGUXutL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as NLP\n",
        "\n",
        "topic = pd.read_csv('topic_with_ner.csv')\n",
        "inputdata = pd.read_csv('inputdata_with_ner.csv')\n",
        "\n",
        "def max_distribution(s):\n",
        "    return topic.topicWords.apply(lambda x: x.replace('[', '').replace(']', '').replace(',', '').replace(\"'\", '').split(' ')).apply(lambda x: sum([1 for p in x if p in s]) / len(x)).idxmax()\n",
        "def percentage(s):\n",
        "    return topic.topicWords.apply(lambda x: x.replace('[', '').replace(']', '').replace(',', '').replace(\"'\", '').split(' ')).apply(lambda x: sum([1 for p in x if p in s]) / len(x)).max()\n",
        "\n",
        "inputdata['topic'] = inputdata.text.apply(lambda x: max_distribution(x))\n",
        "inputdata['percentage'] = inputdata.text.apply(lambda x: percentage(x))\n",
        "\n",
        "inputdata[['medical_speciality', 'topic']].groupby('medical_speciality').agg(list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCvYpuzG_N2z"
      },
      "outputs": [],
      "source": [
        "inputdata[['medical_speciality', 'topic']].groupby('medical_speciality').agg(pd.Series.mode)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqb3uZqPIrA5"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2GGoEwViluNu"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "NER_kelompok_2.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}